{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf31905-bd8b-4a51-8e47-c598532fc503",
   "metadata": {},
   "source": [
    "# Read and Write files from S3 with  Pyspark Container"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9214155-0ed2-4140-af6b-44d590925a08",
   "metadata": {},
   "source": [
    "## Step 1 Geeting the AWS credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b1a1a8-6a35-4f17-a879-c324c91a5b04",
   "metadata": {},
   "source": [
    "A simple way but without the envirooment you can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217949b4-692f-4710-a6b9-3e4031d881e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We assume that you have added your credential with $ aws configure\n",
    "def get_aws_credentials():\n",
    "    with open(os.path.expanduser(\"~/.aws/credentials\")) as f:\n",
    "        for line in f:\n",
    "            #print(line.strip().split(' = '))\n",
    "            try:\n",
    "                key, val = line.strip().split(' = ')\n",
    "                if key == 'aws_access_key_id':\n",
    "                    aws_access_key_id = val\n",
    "                elif key == 'aws_secret_access_key':\n",
    "                    aws_secret_access_key = val\n",
    "            except ValueError:\n",
    "                pass\n",
    "    return aws_access_key_id, aws_secret_access_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5c6e055-7ad4-4fc1-a313-7935a2d823b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key, secret_key = get_aws_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481aea2-f901-4458-9241-603d2e91d1af",
   "metadata": {},
   "source": [
    "For normal use we can export AWS CLI Profile to Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "150a235a-ef64-4413-80ad-98cc7cc11b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "!export AWS_ACCESS_KEY_ID=$(aws configure get default.aws_access_key_id)\n",
    "!export AWS_SECRET_ACCESS_KEY=$(aws configure get default.aws_secret_access_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024d8a08-2f4f-40aa-9355-80d09e1bbddd",
   "metadata": {},
   "source": [
    "and later load the enviroment variables in python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08aec5d-2f0c-4c49-bc47-9c0e591d2fcb",
   "metadata": {},
   "source": [
    "## Step 2 Setup of Hadoop of the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af3b107c-5d3c-4e9d-8f6b-43e95bde22eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47f8563-4003-40a7-8a73-464a0965cd6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/09/01 14:38:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/09/01 14:38:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark S3 reader\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "736243ea-d1f2-4246-9d00-ae55d02b8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext\n",
    "\n",
    "# remove this block if use core-site.xml and env variable\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsAccessKeyId\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsAccessKeyId\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.awsSecretAccessKey\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.awsSecretAccessKey\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3.impl\", \"org.apache.hadoop.fs.s3.S3FileSystem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec139311-4143-4758-a06c-8c6e8ce880e6",
   "metadata": {},
   "source": [
    "## Step 3  Download you demo Dataset to the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8338aac3-c699-4865-9835-e920e4a07309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-01 14:38:46--  https://github.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/raw/master/example/AMZN.csv\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/master/example/AMZN.csv [following]\n",
      "--2022-09-01 14:38:46--  https://raw.githubusercontent.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/master/example/AMZN.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20032 (20K) [text/plain]\n",
      "Saving to: ‘AMZN.csv.4’\n",
      "\n",
      "AMZN.csv.4          100%[===================>]  19.56K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2022-09-01 14:38:46 (6.42 MB/s) - ‘AMZN.csv.4’ saved [20032/20032]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/raw/master/example/AMZN.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333f723-188c-4570-996b-5c86bf22ba7d",
   "metadata": {},
   "source": [
    "## Step 3 Read the dataset present on local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a249058-2741-47a7-8654-5aba4d57ac59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|  Adj Close| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2020-02-10| 2085.01001|2135.600098|2084.959961|2133.909912|2133.909912|5056200|\n",
      "|2020-02-11|2150.899902|2185.949951|     2136.0|2150.800049|2150.800049|5746000|\n",
      "|2020-02-12|2163.199951|    2180.25|2155.290039|     2160.0|     2160.0|3334300|\n",
      "|2020-02-13| 2144.98999|2170.280029|     2142.0|2149.870117|2149.870117|3031800|\n",
      "|2020-02-14|2155.679932|2159.040039|2125.889893|2134.870117|2134.870117|2606200|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_AMZN=spark.read.csv('AMZN.csv',header=True,inferSchema=True)\n",
    "df_AMZN.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ccc9e-ffe4-450d-8062-dfa52aba7b50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 4 Creation of the S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6599f0-3629-4191-bb5f-a048848a64c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d04c3e06-837a-41b4-806c-5b20f1ffddc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s3.Bucket(name='stock-prices-pyspark')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = boto3.resource('s3')\n",
    "# You should change the name the new bucket\n",
    "my_new_bucket='stock-prices-pyspark'\n",
    "s3.create_bucket(Bucket=my_new_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7999e92b-8894-41ad-b956-24aba74c1334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-08-31 21:59:41 stock-prices-pyspark\n"
     ]
    }
   ],
   "source": [
    "# You can list you latest Bucket Created \n",
    "!aws s3 ls --recursive | sort | tail -n 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2395af-6005-4db4-8276-b55d9c19a98b",
   "metadata": {},
   "source": [
    "## Step 5. Write PySpark Dataframe to AWS S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc27591-5f80-4067-9d63-35b9943568ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/01 14:39:02 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "22/09/01 14:39:06 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "22/09/01 14:39:08 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_AMZN.write.format('csv').option('header','true').save('s3a://stock-prices-pyspark/csv/AMZN.csv',mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaf8d95-e77e-43eb-b8c0-c50ab359df11",
   "metadata": {},
   "source": [
    "## Step 6. Read Data from AWS S3 into PySpark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbbbb7bd-cd5d-4bcc-a952-5c9da48ee84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|      Date|       Open|       High|        Low|      Close|  Adj Close| Volume|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "|2020-02-10| 2085.01001|2135.600098|2084.959961|2133.909912|2133.909912|5056200|\n",
      "|2020-02-11|2150.899902|2185.949951|     2136.0|2150.800049|2150.800049|5746000|\n",
      "|2020-02-12|2163.199951|    2180.25|2155.290039|     2160.0|     2160.0|3334300|\n",
      "|2020-02-13| 2144.98999|2170.280029|     2142.0|2149.870117|2149.870117|3031800|\n",
      "|2020-02-14|2155.679932|2159.040039|2125.889893|2134.870117|2134.870117|2606200|\n",
      "+----------+-----------+-----------+-----------+-----------+-----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s3_df=spark.read.csv(\"s3a://stock-prices-pyspark/csv/AMZN.csv\",header=True,inferSchema=True)\n",
    "s3_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e79ceb1-00cb-4ee5-9a61-e2ce7aa4bcf0",
   "metadata": {},
   "source": [
    "## Step 7.  Read the files in the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e8964a9-7d32-4881-991a-be3da15b49c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/csv/GOOG.csv\n",
      "csv/AMZN.csv/_SUCCESS\n",
      "csv/AMZN.csv/part-00000-2f15d0e6-376c-4e19-bbfb-5147235b02c7-c000.csv\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "bucket = \"stock-prices-pyspark\"\n",
    "# We read the files in the Bucket\n",
    "s3 = boto3.resource('s3')\n",
    "my_bucket = s3.Bucket(bucket)\n",
    "for file in my_bucket.objects.all():\n",
    "    print(file.key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9af5ae-5b0a-4b89-a4be-4d1f4cd38412",
   "metadata": {},
   "source": [
    "## Step 8. Read Data from AWS S3 with boto3\n",
    "If you dont need use Pyspark also you can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e0d36fe-5288-45b1-9d1f-29af2214a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We select one file of the bucket\n",
    "bucket = \"stock-prices-pyspark\"\n",
    "file_name = \"csv/AMZN.csv/part-00000-2f15d0e6-376c-4e19-bbfb-5147235b02c7-c000.csv\"\n",
    "#s3 = boto3.client('s3') \n",
    "s3 =  boto3.client('s3', region_name='us-east-1')\n",
    "# 's3' is a key word. create connection to S3 using default config and all buckets within S3\n",
    "obj = s3.get_object(Bucket= bucket, Key= file_name) \n",
    "# get object and file (key) from bucket\n",
    "df = pd.read_csv(obj['Body']) # 'Body' is a key word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "020c4e72-0e3e-4895-8170-90608e1cf823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-02-10</td>\n",
       "      <td>2085.010010</td>\n",
       "      <td>2135.600098</td>\n",
       "      <td>2084.959961</td>\n",
       "      <td>2133.909912</td>\n",
       "      <td>2133.909912</td>\n",
       "      <td>5056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>2150.899902</td>\n",
       "      <td>2185.949951</td>\n",
       "      <td>2136.000000</td>\n",
       "      <td>2150.800049</td>\n",
       "      <td>2150.800049</td>\n",
       "      <td>5746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-02-12</td>\n",
       "      <td>2163.199951</td>\n",
       "      <td>2180.250000</td>\n",
       "      <td>2155.290039</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>2160.000000</td>\n",
       "      <td>3334300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-02-13</td>\n",
       "      <td>2144.989990</td>\n",
       "      <td>2170.280029</td>\n",
       "      <td>2142.000000</td>\n",
       "      <td>2149.870117</td>\n",
       "      <td>2149.870117</td>\n",
       "      <td>3031800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-02-14</td>\n",
       "      <td>2155.679932</td>\n",
       "      <td>2159.040039</td>\n",
       "      <td>2125.889893</td>\n",
       "      <td>2134.870117</td>\n",
       "      <td>2134.870117</td>\n",
       "      <td>2606200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date         Open         High          Low        Close  \\\n",
       "0  2020-02-10  2085.010010  2135.600098  2084.959961  2133.909912   \n",
       "1  2020-02-11  2150.899902  2185.949951  2136.000000  2150.800049   \n",
       "2  2020-02-12  2163.199951  2180.250000  2155.290039  2160.000000   \n",
       "3  2020-02-13  2144.989990  2170.280029  2142.000000  2149.870117   \n",
       "4  2020-02-14  2155.679932  2159.040039  2125.889893  2134.870117   \n",
       "\n",
       "     Adj Close   Volume  \n",
       "0  2133.909912  5056200  \n",
       "1  2150.800049  5746000  \n",
       "2  2160.000000  3334300  \n",
       "3  2149.870117  3031800  \n",
       "4  2134.870117  2606200  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a974aef7-2924-48e4-b1bf-694ddaa3993a",
   "metadata": {},
   "source": [
    "## Step 9 - Downloading Multiple Files locally with wget\n",
    "If you want to download multiple files at once, use the -i option followed by the path to a local or external file containing a list of the URLs to be downloaded. Each URL needs to be on a separate line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8052b6d7-af39-4aa1-9a6f-df5054a7e7da",
   "metadata": {},
   "source": [
    "We create the file list to download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58a44459-6847-4709-9573-ea5ea2553805",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"datasets.txt\",\"a\") as file:\n",
    "    file.write(\"https://github.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/raw/master/example/AMZN.csv\\n\")\n",
    "    file.write(\"https://github.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/raw/master/example/GOOG.csv\\n\")\n",
    "    file.write(\"https://github.com/ruslanmv/How-to-read-and-write-files-in-S3-from-Pyspark-Docker/raw/master/example/TSLA.csv\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f3b91b7e-be9a-4beb-9c75-a5e6cc7ed63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we download all the files\n",
    "!wget -q -i datasets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d45fb764-4c64-4792-9dce-1555d68bdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "bucket = \"stock-prices-pyspark\"\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file('GOOG.csv', bucket, 'csv/'+'GOOG.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbf96418-6881-44ba-9bc7-dac1e567c2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-24 14:42:55 datalake-temporal-proyect-240122\n",
      "2022-07-25 21:31:05 mysound-s3-ruslanmv\n",
      "2022-08-03 19:50:23 sagemaker-studio-342527032693-bas5sukiu4c\n",
      "2022-08-31 21:59:41 stock-prices-pyspark\n",
      "2022-08-31 21:22:06 stock-prices-spark\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "68cddedf-3c28-4472-9b60-418b9f8c1f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
